{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#set directory\n",
    "os.chdir('/Users/eworkman/Documents/nlp-reportsv2/nlp-reports/nlp-reports-v2')\n",
    "\n",
    "CONFIGFILE = 'trial/config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To run this pipeline:\n",
    "\n",
    "- spaCy version greater than 3.0 neeeded\n",
    "\n",
    "- ensure en_core_sci_scibert-0.4.0.tar is installed in a location your env can find. follow download instructions at https://github.com/allenai/scispacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from conceptextract import keywds as kw\n",
    "from conceptextract import path_fun as pf\n",
    "from conceptextract import helper_fun as hf\n",
    "from conceptextract import make_rules_from_config as mr\n",
    "import yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#first initialize rule parameters object that contain all settings needed to run nlp pipeline\n",
    "rp = mr.RuleParameters(CONFIGFILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load nlp negex\n",
      "the pseudo_negations for this session is: ['no further', 'not able to be', 'not certain if', 'not certain whether', 'not necessarily', 'without any further', 'without difficulty', 'without further', 'might not', 'not only', 'no increase', 'no significant change', 'no change', 'no definite change', 'not extend', 'not cause', 'not certain if', 'not certain whether', 'gram negative', 'not rule out', 'not ruled out', 'not been ruled out', 'not drain', 'no suspicious change', 'no interval change', 'no significant interval change', 'with no improvement at rest', 'with no improvement on rest', 'not improve at rest', 'with no reversibility at rest', 'with no improvement on the rest study', 'with no significant improvement on rest imaging', 'with no reversibility on rest']\n",
      "\n",
      "\n",
      "the preceding_negations for this session is: ['absence of', 'declined', 'denied', 'denies', 'denying', 'no sign of', 'no signs of', 'not', 'not demonstrate', 'symptoms atypical', 'doubt', 'negative for', 'no', 'versus', 'without', \"doesn't\", 'doesnt', \"don't\", 'dont', \"didn't\", 'didnt', \"wasn't\", 'wasnt', \"weren't\", 'werent', \"isn't\", 'isnt', \"aren't\", 'arent', 'cannot', \"can't\", 'cant', \"couldn't\", 'couldnt', 'never', 'patient was not', 'without indication of', 'without sign of', 'without signs of', 'without any reactions or signs of', 'no complaints of', 'no evidence of', 'no cause of', 'evaluate for', 'fails to reveal', 'free of', 'never developed', 'never had', 'did not exhibit', 'rules out', 'rule out', 'rule him out', 'rule her out', 'rule patient out', 'rule the patient out', 'ruled out', 'ruled him out', 'ruled her out', 'ruled patient out', 'ruled the patient out', 'r/o', 'ro', 'with no improvement at rest', 'with no improvement on rest', 'not improve at rest', 'with no reversibility at rest', 'with no improvement on the rest study', 'with no significant improvement on rest imaging', 'with no reversibility on rest']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eworkman/miniconda3/envs/test_package/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "#run spacy pipeline and save objects \n",
    "#this only needs to be loaded once per kernel session. \n",
    "\n",
    "\n",
    "print('load nlp negex')\n",
    "nlp = kw.load_negex_model_ud(model = rp.nlp_cfg['Language_model'], \n",
    "                       language = 'en_clinical', \n",
    "                       chunks = [\"no\"], \n",
    "                       pseudo_negations= rp.nlp_cfg['Pseudo_negations'],\n",
    "                      preceding_negations=rp.nlp_cfg['Pseudo_negations'],\n",
    "                      entity_pats =  rp.nlp_cfg['Entity_Patterns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial/inputs created\n",
      "ischemi(a|c)|scar|lcx_disease|transmural|nontransmural|infarction|infarct\n",
      "\n",
      "processing  Bullet3 with filter  ischemi(a|c)|scar|lcx_disease|transmural|nontransmural|infarction|infarct\n",
      "stream nlp process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:14,  6.84it/s]\n",
      "100it [00:00, 30146.65it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [00:06, 19.29it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked sents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:14,  6.67it/s]\n",
      "100it [00:00, 32256.43it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked lemmas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "116it [00:06, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked lemma sents\n",
      "docs\n",
      "docs no of docs 100\n",
      "docs len keys 100\n",
      "docs_lemma\n",
      "docs_lemma no of docs 100\n",
      "docs_lemma len keys 100\n",
      "docs_sentences\n",
      "docs_sentences no of docs 116\n",
      "docs_sentences len keys 116\n",
      "docs_sentences_lemmas\n",
      "docs_sentences_lemmas no of docs 116\n",
      "docs_sentences_lemmas len keys 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['docs', 'docs_lemma', 'docs_sentences', 'docs_sentences_lemmas'])\n",
      "processing  Bullet1 with filter  \n",
      "stream nlp process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:03, 28.87it/s]\n",
      "100it [00:00, 104936.30it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [00:02, 38.40it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked sents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:03, 30.58it/s]\n",
      "100it [00:00, 70766.05it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked lemmas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [00:02, 38.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunked lemma sents\n",
      "docs\n",
      "docs no of docs 100\n",
      "docs len keys 100\n",
      "docs_lemma\n",
      "docs_lemma no of docs 100\n",
      "docs_lemma len keys 100\n",
      "docs_sentences\n",
      "docs_sentences no of docs 105\n",
      "docs_sentences len keys 105\n",
      "docs_sentences_lemmas\n",
      "docs_sentences_lemmas no of docs 105\n",
      "docs_sentences_lemmas len keys 105\n",
      "dict_keys(['docs', 'docs_lemma', 'docs_sentences', 'docs_sentences_lemmas'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_df = pd.read_csv(os.path.join(rp.FILE_PATHS['data_directory'],rp.DATA_INFO['data_csv']))\n",
    "\n",
    "if rp.FILE_PATHS['code_path_library']:\n",
    "    pf.insert_nlp_lib_to_path(rp.FILE_PATHS['code_path_library'])\n",
    "pf.create_dir(rp.FILE_PATHS['input_directory'])\n",
    "\n",
    "tp = rp.DATA_INFO['data_columns_with_keywd_filter']\n",
    "\n",
    "#checking the config is as expected\n",
    "for column, filt in rp.DATA_INFO['data_columns_with_keywd_filter']:\n",
    "    print(filt)\n",
    "\n",
    "#run spacy pipeline and save to pickles for reuse by rule set and save 1 pickle object per data column processed\n",
    "\n",
    "\n",
    "dicts = hf.run_nlp_pipeline(nlp, df = data_df, texts_to_parse = tp, study_index_col = rp.DATA_INFO['index_column'] ,\n",
    "                         outputDir = rp.FILE_PATHS['input_directory'], b= 100, b2 =500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook run_nlp_pipeline_step1.ipynb to script\n",
      "[NbConvertApp] Writing 2164 bytes to run_nlp_pipeline_step1.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script run_nlp_pipeline_step1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
