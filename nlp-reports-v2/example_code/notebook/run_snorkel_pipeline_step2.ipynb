{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946fa3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b7fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#set directory\n",
    "os.chdir('/Users/eworkman/Documents/nlp-reportsv2/nlp-reports/nlp-reports-v2')\n",
    "\n",
    "CONFIGFILE = 'trial/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3524db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from conceptextract import keywds as kw\n",
    "from conceptextract import path_fun as pf\n",
    "from conceptextract import helper_fun as hf\n",
    "from conceptextract import make_rules_from_config as mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f5acd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load nlp negex\n",
      "the pseudo_negations for this session is: ['no further', 'not able to be', 'not certain if', 'not certain whether', 'not necessarily', 'without any further', 'without difficulty', 'without further', 'might not', 'not only', 'no increase', 'no significant change', 'no change', 'no definite change', 'not extend', 'not cause', 'not certain if', 'not certain whether', 'gram negative', 'not rule out', 'not ruled out', 'not been ruled out', 'not drain', 'no suspicious change', 'no interval change', 'no significant interval change', 'with no improvement at rest', 'with no improvement on rest', 'not improve at rest', 'with no reversibility at rest', 'with no improvement on the rest study', 'with no significant improvement on rest imaging', 'with no reversibility on rest']\n",
      "\n",
      "\n",
      "the preceding_negations for this session is: ['absence of', 'declined', 'denied', 'denies', 'denying', 'no sign of', 'no signs of', 'not', 'not demonstrate', 'symptoms atypical', 'doubt', 'negative for', 'no', 'versus', 'without', \"doesn't\", 'doesnt', \"don't\", 'dont', \"didn't\", 'didnt', \"wasn't\", 'wasnt', \"weren't\", 'werent', \"isn't\", 'isnt', \"aren't\", 'arent', 'cannot', \"can't\", 'cant', \"couldn't\", 'couldnt', 'never', 'patient was not', 'without indication of', 'without sign of', 'without signs of', 'without any reactions or signs of', 'no complaints of', 'no evidence of', 'no cause of', 'evaluate for', 'fails to reveal', 'free of', 'never developed', 'never had', 'did not exhibit', 'rules out', 'rule out', 'rule him out', 'rule her out', 'rule patient out', 'rule the patient out', 'ruled out', 'ruled him out', 'ruled her out', 'ruled patient out', 'ruled the patient out', 'r/o', 'ro', 'with no improvement at rest', 'with no improvement on rest', 'not improve at rest', 'with no reversibility at rest', 'with no improvement on the rest study', 'with no significant improvement on rest imaging', 'with no reversibility on rest']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eworkman/miniconda3/envs/test_package/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    }
   ],
   "source": [
    "#load spacy pipeline\n",
    "#this only needs to be loaded once per kernel\n",
    "rp_nlp = mr.RuleParameters(CONFIGFILE)\n",
    "\n",
    "print('load nlp negex')\n",
    "nlp = kw.load_negex_model_ud(model = rp_nlp.nlp_cfg['Language_model'], \n",
    "                       language = 'en_clinical', \n",
    "                       chunks = [\"no\"], \n",
    "                       pseudo_negations= rp_nlp.nlp_cfg['Pseudo_negations'],\n",
    "                      preceding_negations=rp_nlp.nlp_cfg['Pseudo_negations'],\n",
    "                      entity_pats =  rp_nlp.nlp_cfg['Entity_Patterns'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cedef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating rules parameter object for 8 labeling functions for 1 concept ruleset Overall with base text in Bullet1\n",
      "these are the pickles to unpickle:  ['trial/inputs/Bullet1_nlp_objects.pickle']\n",
      "processing nlp objects and nlp object keys\n",
      "reading in docs of len 100 from docs\n",
      "reading in docs of len 100 from docs_lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 8668.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in docs of len 105 from docs_sentences\n",
      "reading in docs of len 105 from docs_sentences_lemmas\n",
      "creating analytic df for Bullet1  with  ['']\n",
      "docs_sentences  has lens ids: 105  & text: 105\n",
      "docs_sentences_lemmas  has lens ids: 105  & text: 105\n",
      "running snorkel pipeline\n",
      "running Interp2 rules\n",
      "Index(['Study_No', 'Bullet1', 'Interp2', 'docs_text', 'sentence_list_text',\n",
      "       'docs_lemma', 'sentence_list_lemma', 'nlp_string', 'lemma_string'],\n",
      "      dtype='object')\n",
      "number of non identical codes out of labeled set for Interp2\n",
      "(2, 10)\n",
      "Interp2: precision: 0.9827586206896551 , recall: 0.5816326530612245, f1: 0.7307692307692307\n",
      "Overall_data_predicted.csv, Overall_prf.csv and internal_metrics.csvs created in trial/outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Run\n",
    "#first initialize rule parameters that contain all settings needed for a given rule set\n",
    "\n",
    "RULESET = 'Overall'\n",
    "CONCEPT_NUM = 1\n",
    "\n",
    "\n",
    "rp = mr.RuleParameters(CONFIGFILE,rule_set='Overall', num = 1)\n",
    "\n",
    "snorkelrules =rp.run_LF_maker()\n",
    "\n",
    "#parameters for the snorkel run. \n",
    "\n",
    "input_path = rp.FILE_PATHS['input_directory']\n",
    "data_df = pd.read_csv(os.path.join(rp.FILE_PATHS['data_directory'],rp.DATA_INFO['data_csv']))\n",
    "data_column = rp.metadata['data_column']\n",
    "index_column =rp.DATA_INFO['index_column'] \n",
    "eval_columns = [index_column, data_column] +list(snorkelrules.keys())\n",
    "filt = rp.sent_filt\n",
    "retokenization_list = rp.retokenization_list\n",
    "filePat = rp.metadata['out_prefix']\n",
    "output_path = rp.FILE_PATHS['output_directory']\n",
    "\n",
    "    \n",
    "#loads previously saved nlp object\n",
    "analytic_df = hf.load_nlp_pickles_to_df(nlp, input_path, data_df, data_column,\n",
    "                                         eval_columns, filt, retokenization_list, index_column)\n",
    "    \n",
    "#runs snorkel pipeline creating precision and recall outputs\n",
    "hf.run_snorkel_pipeline(snorkelrules, analytic_df, filePat, output_path, GoldStd = True)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9945465-b3f0-44c0-9e42-e0c93152c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating rules parameter object for 8 labeling functions for 2 concept ruleset IschemiaVessel with base text in Bullet3\n",
      "making LF dict for Ischemia_RCA with concepts ['ischemia', 'ischemic'] and                 ['rca', 'rca territory', 'rca territories', 'rca/lcx', 'lcx/rca']\n",
      "making LF dict for Ischemia_LAD with concepts ['ischemia', 'ischemic'] and                 ['lad', 'lad territory', 'lad territories']\n",
      "making LF dict for Ischemia_LCX with concepts ['ischemia', 'ischemic'] and                 ['lcx', 'lcx territory', 'lcx territories', 'lcx_disease', 'rca/lcx', 'lcx/rca', 'lcx/diagonal', 'circumflex']\n",
      "these are the pickles to unpickle:  ['trial/inputs/Bullet3_nlp_objects.pickle']\n",
      "processing nlp objects and nlp object keys\n",
      "reading in docs of len 100 from docs\n",
      "reading in docs of len 100 from docs_lemma\n",
      "reading in docs of len 116 from docs_sentences\n",
      "reading in docs of len 116 from docs_sentences_lemmas\n",
      "creating analytic df for Bullet3  with  ['ischemi(a|c)|scar|lcx_disease|transmural|nontransmural|infarction|infarct']\n",
      "docs_sentences  has lens ids: 116  & text: 116\n",
      "docs_sentences_lemmas  has lens ids: 116  & text: 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 2569.90it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 2734.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3056.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running snorkel pipeline\n",
      "running Ischemia_RCA rules\n",
      "Index(['Study_No', 'Bullet3', 'Ischemia_RCA', 'Ischemia_LAD', 'Ischemia_LCX',\n",
      "       'docs_text', 'sentence_list_text', 'docs_lemma', 'sentence_list_lemma',\n",
      "       'nlp_string', 'lemma_string'],\n",
      "      dtype='object')\n",
      "number of non identical codes out of labeled set for Ischemia_RCA\n",
      "(1, 12)\n",
      "Ischemia_RCA: precision: 1.0 , recall: 1.0, f1: 1.0\n",
      "running Ischemia_LAD rules\n",
      "Index(['Study_No', 'Bullet3', 'Ischemia_RCA', 'Ischemia_LAD', 'Ischemia_LCX',\n",
      "       'docs_text', 'sentence_list_text', 'docs_lemma', 'sentence_list_lemma',\n",
      "       'nlp_string', 'lemma_string', 'predicted_Ischemia_RCA'],\n",
      "      dtype='object')\n",
      "number of non identical codes out of labeled set for Ischemia_LAD\n",
      "(0, 13)\n",
      "Ischemia_LAD: precision: 0.9411764705882353 , recall: 1.0, f1: 0.9696969696969697\n",
      "running Ischemia_LCX rules\n",
      "Index(['Study_No', 'Bullet3', 'Ischemia_RCA', 'Ischemia_LAD', 'Ischemia_LCX',\n",
      "       'docs_text', 'sentence_list_text', 'docs_lemma', 'sentence_list_lemma',\n",
      "       'nlp_string', 'lemma_string', 'predicted_Ischemia_RCA',\n",
      "       'predicted_Ischemia_LAD'],\n",
      "      dtype='object')\n",
      "number of non identical codes out of labeled set for Ischemia_LCX\n",
      "(0, 14)\n",
      "Ischemia_LCX: precision: 0.9 , recall: 1.0, f1: 0.9473684210526316\n",
      "IschemiaVessel_data_predicted.csv, IschemiaVessel_prf.csv and internal_metrics.csvs created in trial/outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#dual concept example\n",
    "\n",
    "RULESET = 'IschemiaVessel'\n",
    "CONCEPT_NUM = 2\n",
    "#first initialize rule parameters that contain all settings needed for a given rule set\n",
    "rp = mr.RuleParameters(CONFIGFILE,rule_set=RULESET, num = CONCEPT_NUM )\n",
    "\n",
    "#Run\n",
    "\n",
    "snorkelrules =rp.run_LF_maker()\n",
    "\n",
    "#parameters for the snorkel run. \n",
    "input_path = rp.FILE_PATHS['input_directory']\n",
    "data_df = pd.read_csv(os.path.join(rp.FILE_PATHS['data_directory'],rp.DATA_INFO['data_csv']))\n",
    "data_column = rp.metadata['data_column']\n",
    "index_column =rp.DATA_INFO['index_column'] \n",
    "eval_columns = [index_column, data_column] +list(snorkelrules.keys())\n",
    "filt = rp.sent_filt\n",
    "retokenization_list = rp.retokenization_list\n",
    "\n",
    "filePat = rp.metadata['out_prefix']\n",
    "output_path = rp.FILE_PATHS['output_directory']\n",
    "\n",
    "    \n",
    "#loads previously saved nlp object\n",
    "analytic_df = hf.load_nlp_pickles_to_df(nlp, input_path, data_df, data_column,\n",
    "                                         eval_columns, filt, retokenization_list, index_column)\n",
    "    \n",
    "#runs snorkel pipeline creating precision and recall outputs\n",
    "hf.run_snorkel_pipeline(snorkelrules, analytic_df, filePat, output_path, GoldStd = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa263fd-be22-409c-b462-6de9657ff399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook run_snorkel_pipeline_step2.ipynb to script\n",
      "[NbConvertApp] Writing 3219 bytes to run_snorkel_pipeline_step2.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script run_snorkel_pipeline_step2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e6ad7-86e8-4773-87d4-d6de82dc710f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
